# Bias Detection in LLM Data Narratives (Task 08)

## Overview

This project examines whether Large Language Models (LLMs) display cognitive biases when they interpret the same dataset under different prompt conditions. The study uses anonymized statistics from the *International Football Results* dataset (1872–2017) and evaluates how prompt structure influences the narratives generated by multiple LLMs.

The goal is to understand if small changes in prompt framing can lead to systematically different interpretations, similar to human cognitive biases.

---

## Research Question

**Do LLMs produce different narratives from identical data when that data is presented using subtly different frames, orders, or metrics?**

This question matters because LLMs are widely used for data interpretation in journalism, research, business intelligence, and education. If their outputs change due to framing rather than actual data, users may unknowingly receive biased insights.

---

## What the Project Does

1. **Creates paired prompts** that test five cognitive bias types:

   * Framing bias
   * Anchoring bias
   * Attribution bias
   * Selection bias
   * Metric/definition bias

2. **Uses anonymized team names** (Team A–E) to avoid cultural associations.

3. **Collects responses** from three LLMs:

   * GPT-5
   * Claude 3 Opus
   * Gemini 1.5 Pro

4. **Analyzes LLM outputs** using:

   * Sentiment analysis
   * Statistical tests (t-tests, chi-square, effect sizes)
   * Numerical claim validation
   * Visualizations

5. **Documents the full experimental design**, including reproducibility steps, ground truth values, and ethical considerations.

---

## Ground Truth Dataset (Anonymized)

The project uses aggregated statistics derived from the Kaggle football dataset. The five highest-activity teams are anonymized:

| Team   | Win Rate (%) |
| ------ | ------------ |
| Team A | 63.5         |
| Team B | 59.2         |
| Team C | 58.7         |
| Team D | 57.8         |
| Team E | 55.1         |

**Other fixed values used in prompts:**

* Average goals per match: 2.75
* Home advantage: +0.49 goals

These fixed values ensure that all LLMs are reacting to the same data.

---

## Hypotheses Tested

Each hypothesis has two prompt variants (A and B). Together, they test whether prompt design can induce bias:

| ID | Bias Type         | Manipulation                                                 |
| -- | ----------------- | ------------------------------------------------------------ |
| H1 | Framing           | "Growth potential" vs. "underperformance" wording            |
| H2 | Anchoring         | High-to-low vs. low-to-high order of team statistics         |
| H3 | Attribution       | “What went wrong” vs. “What are opportunities”               |
| H4 | Selection         | Entire dataset vs. friendlies-only (friendlies not provided) |
| H5 | Metric/Definition | “Dominant” vs. “consistent” (undefined metrics)              |

---

## Project Structure

```
Task08_Bias_Detection/
│
├── README.md
├── REPORT.md
├── EXPERIMENTAL_DESIGN.md
├── COMPLETION_SUMMARY.md
├── requirements.txt
│
├── data/
│   └── results.csv            # Kaggle dataset (manually downloaded)
│
├── prompts/
│   └── variants.json          # Generated experimental prompt pairs
│
├── results/
│   ├── logs_csv/              # CSV logs of LLM responses
│   ├── logs_jsonl/            # JSONL logs of LLM responses
│   ├── summaries/             # Claim validation & aggregation reports
│   └── analysis/              # Visualizations & statistics
│
└── src/
    ├── experiment_design.py
    ├── run_experiment.py
    ├── analyze_bias.py
    └── validate_claims.py
```

---

## Installation

### 1. Clone the repository

```
git clone <your-repo-url>
cd Task08_Bias_Detection
```

### 2. Create a Python environment

```
python3.10 -m venv .venv
source .venv/bin/activate         # macOS/Linux
# .venv\Scripts\activate          # Windows
```

### 3. Install dependencies

```
pip install --upgrade pip
pip install -r requirements.txt
```

### 4. Download the dataset

Download the **International Football Results (1872–2017)** dataset from Kaggle
Place `results.csv` into:

```
data/results.csv
```

Verify:

```
python -c "import pandas as pd; print(len(pd.read_csv('data/results.csv')))"
```

Expected: ~40,000 rows.

---

## Running the Experiment

### 1. Generate paired prompts

```
python src/experiment_design.py
```

This creates:
`prompts/variants.json`

### 2. Collect LLM responses

Manual interactive collection:

```
python src/run_experiment.py --model claude-opus --model-version "claude-3-opus-20240229" --runs 5
```

The script:

* Shows a prompt
* You paste it into the LLM
* Paste the response back
* Logs to:

  * `results/logs_csv/`
  * `results/logs_jsonl/`

### 3. Analyze bias patterns

```
python src/analyze_bias.py results/logs_jsonl/<your-file>.jsonl
```

Performs:

* Sentiment analysis
* Statistical testing
* Effect size calculation
* Visualization generation

### 4. Validate numerical claims

```
python src/validate_claims.py results/logs_jsonl/<your-file>.jsonl
```

Outputs:

* Claim validation report
* Error counts
* Mismatch details

---

## Expected Outcomes

Based on cognitive bias literature:

* **H1**: Win-framed prompts should produce more positive language.
* **H2**: High-to-low ordering may anchor LLMs into stronger team rankings.
* **H3**: Problem-focused prompts may generate stronger negative evaluations.
* **H4**: Missing data (friendlies) may cause refusal or cautious output.
* **H5**: Ambiguous terms like “dominance” and “consistency” may produce different interpretations even with identical data.

Final measured results will be added to `REPORT.md` after data collection.

---

## Ethical Considerations

* No personal data is used.
* All team names are anonymized (Team A–E).
* All data is publicly available.
* No deceptive prompts are used.
* The goal is to understand bias, not exploit it.
* The project is IRB-exempt due to non-human data.

---

## Troubleshooting

**ModuleNotFoundError: python-dotenv**
→ Optional. Install or ignore.

**Apple Silicon dependency problems**
→ Always use a virtual environment.

**Empty JSONL analysis**
→ Check that the file path is correct and logs contain valid lines.

**TextBlob missing corpora**

```
python -m textblob.download_corpora
```

**scipy yanked version**

```
pip install scipy==1.11.4
```
